<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Tutorial:Regression from Scratch and Simple  &middot; rTales: DataScience in Short</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="regression, words, ">


<meta property="og:title" content="Tutorial:Regression from Scratch and Simple  &middot; rTales: DataScience in Short ">
<meta property="og:site_name" content="rTales: DataScience in Short"/>
<meta property="og:url" content="/regression-from-scratch-and-simple/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2020-02-09T00:00:00Z" />
<meta property="og:article:modified_time" content="2020-02-09T00:00:00Z" />

  
    
<meta property="og:article:tag" content="regression">
    
<meta property="og:article:tag" content="words">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@@neuronalnet" />
<meta name="twitter:creator" content="@@neuronalnet" />
<meta name="twitter:title" content="Tutorial:Regression from Scratch and Simple" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="/regression-from-scratch-and-simple/" />
<meta name="twitter:domain" content="/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Tutorial:Regression from Scratch and Simple",
    "author": {
      "@type": "Person",
      "name": "Alvaro Aguado"
    },
    "datePublished": "2020-02-09",
    "description": "",
    "wordCount":  3099 
  }
</script>



<link rel="canonical" href="/regression-from-scratch-and-simple/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
<link href="/favicon.png" rel="icon" type="image/x-icon" />

<meta name="generator" content="Hugo 0.64.0" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-145400836-1', 'auto');
	  ga('send', 'pageview');

	</script>

</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <img src="/rTales-logo.png">
  <a class="baselink" href="/">
  

</a>

</div>

  
<div class="container topline">
  
  Solve practical problems with Data Science and Technology


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="/">Home</a>


  
<a href="/about">About</a>

<a href="/post" title="Show list of posts">Posts</a>

<a href="/resources">Resources</a>

<a href="/tags" title="Show list of tags">Tags</a>


</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" rel="me" aria-label="Email" href="mailto:alvaroaguado3@gmail.com">
  <span class="fa fa-envelope-square"></span></a>



<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/alvaroaguado3">
  <span class="fa fa-github-square"></span></a>




 
<a id="contact-link-linkedin" class="contact_link" rel="me" aria-label="LinkedIn" href="https://www.linkedin.com/in/alvaro-aguado-74314120/">
  <span class="fa fa-linkedin-square"></span></a>







<a id="contact-link-twitter" class="contact_link" rel="me" aria-label="Twitter" href="https://twitter.com/@neuronalnet">
  <span class="fa fa-twitter-square"></span></a>













</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Tutorial:Regression from Scratch and Simple
</h1>

  <div class="metas">
<time datetime="2020-02-09">9 Feb, 2020</time>


  
    &middot; by Alvaro Aguado
  
  &middot; Read in about 15 min
  &middot; (3099 Words)
  <br>
  
<a class="label" href="/tags/regression">Regression</a>

<a class="label" href="/tags/very-easy">Very Easy</a>

<a class="label" href="/tags/linear-algebra">Linear Algebra</a>

<a class="label" href="/tags/least-squares">Least Squares</a>


</div>

</header>

  <div class="container content">
  


<div id="question" class="section level3">
<h3>Question:</h3>
<p>I’m not a math expert. But I want to understand least squares completely. Can you please explain how to do Least Squares Regression from Scratch?</p>
</div>
<div id="solution" class="section level3">
<h3>Solution:</h3>
<p>Let’s see the hard math solution first and then we will walk through to make sure we understand this fully. Revisit this once you are done to see if you get it the second time.</p>
<p>Imagine we have the output variable <span class="math inline">\(y = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\)</span> and the input variables <span class="math inline">\(X = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span> where the first column represents the intercept and the second column is the second variable.</p>
<p>In order to do a least squares we need to find the values of <span class="math inline">\(\beta\)</span> that minimize the residual sum of squares <span class="math inline">\(RSS(\beta) = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j=i}^p x_{ij}\beta_j)^2\)</span> or in matrix notation <span class="math inline">\(RSS(\beta) = (y - X\beta)^T(y-X\beta)\)</span></p>
<p>Differentiating with respect to <span class="math inline">\(\beta\)</span> we obtain <span class="math display">\[\frac{\partial RSS}{\partial\beta} = -2X^T (y - X\beta) \]</span>
If <span class="math inline">\(X\)</span> is full rank then we can set the first derivative to zero <span class="math inline">\(-2X^T (y - X\hat \beta) = 0\)</span>. This zero doesn’t mean that the RSS will be zero, instead it means that the value we find its a minimum for the function. If we isolate <span class="math inline">\(\hat\beta\)</span> we get that <span class="math display">\[\hat \beta = (X^TX)^{-1}X^Ty \]</span></p>
<p>So let’s apply this to our data then</p>
<pre class="r"><code># Create Data
x0 &lt;- matrix(data = c(1,1))
x1 &lt;- matrix(data = c(1,2))
y &lt;- matrix(data = c(2,1))
X &lt;- cbind(x0,x1)

# Regress y on X
betas_X &lt;- solve(t(X) %*% X) %*% t(X) %*% y

## If we compare this to the lm function in R
lm1 &lt;- lm(y ~ X - 1) # Regression using lm function

## If we compare the results
t(betas_X)  # Result manual</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    3   -1</code></pre>
<pre class="r"><code>coef(lm1) # Result lm function</code></pre>
<pre><code>## X1 X2 
##  3 -1</code></pre>
<p>Now can we predict <span class="math inline">\(\hat y\)</span> with <span class="math inline">\(X\hat\beta\)</span>. And check how well we did compared to the original ?</p>
<pre class="r"><code># Multiply the variables by the coefficients
y_hat &lt;- X %*% betas_X

#Let&#39;s see the results from the original
y</code></pre>
<pre><code>##      [,1]
## [1,]    2
## [2,]    1</code></pre>
<pre class="r"><code># And the results from our prediction
y_hat</code></pre>
<pre><code>##      [,1]
## [1,]    2
## [2,]    1</code></pre>
<p>Pretty good! But how did we get here? I mean it’s a simple example but if we get exactly what’s going on we will be able to give a better interpretation to the outputs we get when we do regression in any statistical software.
Let’s do this very slowly to understand what’s happening.</p>
</div>
<div id="tutorial" class="section level3">
<h3>Tutorial:</h3>
<p>Let’s start simple with the same points we talk about before. We will plot this in a graph with x-axis and y-axis</p>
<pre class="r"><code># Create data
x1 &lt;- matrix(data = c(1,2))
y &lt;- matrix(data = c(2,1))

# Plot points
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Here is clear that a regression line will connect the first point (in x1 = 1 and y = 2) and the second point (in x1 = 2 and y = 1). So in our line for every unit of <span class="math inline">\(x1\)</span> (from x = 1 to x = 2) we descend 1 unit of <span class="math inline">\(y\)</span> (from y = 2 to y = 1) and when x = 0 (the intercept) y should be equal to 3. Let’s make that plot, where the intercept = 3 and for each unit of x, y descends one unit (-1)</p>
<pre class="r"><code># Create data
x1 &lt;- matrix(data = c(1,2))
y &lt;- matrix(data = c(2,1))

# Plot points
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)
abline(a = 3,b = -1,col = &#39;red&#39;)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Perfect! But that’s too easy because we only have two points. A line can explain the relationship of two points perfectly by passing through both points. But what happens when there’s a third point that can’t go through all the points? If we used the previous line, that extra point would be separated from the regression line by a small distance. That distance is called residual.</p>
<pre class="r"><code># Create data
x1 &lt;- matrix(data = c(1,2,1.5))
y &lt;- matrix(data = c(2,1,1.75))

# Plot points
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)

# Draw Regression line
abline(a = 3,b = -1,col = &#39;red&#39;)

# Draw Residuals and text
segments(x0 = 1.5,x1 = 1.5,y0 = 1.75, y1 = 1.5,col = &quot;blue&quot;,lwd = 3)
text(1.6,1.6,&quot;Residual &#39;e_2&#39;&quot;,pos = 4)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>If we moved the slope of the regression, then the residuals of the other points would grow. If you see the blue line, that’s the perpendicular from each point to the regression line. Now consider that the value of the red line in the y-axis for every point in the x-axis (x1 variable) it’s our prediction, will refer to it as <span class="math inline">\(\hat y\)</span> (y-hat). You can think of a regression as an input output machine. When I input a <span class="math inline">\(x=1\)</span>, my machine outputs a <span class="math inline">\(\hat y=2\)</span>. When I input a <span class="math inline">\(x=2\)</span>, my machine outputs a <span class="math inline">\(\hat y=1\)</span>, when I input a <span class="math inline">\(x=3\)</span> then my machine produces a <span class="math inline">\(\hat y=0\)</span> and so on.</p>
<p>Now the real values of the <span class="math inline">\(y\)</span> variable are <span class="math inline">\(2,1.75\)</span> and <span class="math inline">\(1\)</span>, and the values of <span class="math inline">\(\hat y\)</span> are <span class="math inline">\(2,1.5\)</span> and <span class="math inline">\(1\)</span>. So the residuals <span class="math inline">\(e_i\)</span> are <span class="math inline">\(e_i = y_i - \hat y_i\)</span> are <span class="math inline">\(2-2 = 0, 1.75 - 1.5 = .25, 1 - 1 = 0\)</span>, so <span class="math inline">\(0,0.25\)</span> and <span class="math inline">\(0\)</span>. If we sum all the values (the sum of the residuals) the total would be <span class="math inline">\(0.25\)</span></p>
<p>Instead, we almost always will want to minimize the residual sum of squares <span class="math inline">\((y - \hat y)^2\)</span>. (By the way the sum of squares in our previous example would be <span class="math inline">\(0^2 + 0.25^2 + 0^2 = 0.0625\)</span>)</p>
<p><strong>Question 1:</strong> Why do we want to minimize the residual sum of squares in regression?</p>
<p><strong>Solution 1:</strong> When we do regression we want to minimize the sum <span class="math inline">\(e_i\)</span>. But if we only sum the residuals then the best regression would be the mean value of <span class="math inline">\(y\)</span> (normally depicted as <span class="math inline">\(\bar y\)</span>). That will not capture the relationship between x and y, becuse it’s a constant, or in the graph a horizontal line. That’s why instead we want to use the residual sum of squares.</p>
<p>Want to see an example? Look at the blue lines (the residuals of each point). If you sum their values, the negatives cancel the positive values, and as a result the best regression line is the mean value of <span class="math inline">\(y\)</span></p>
<pre class="r"><code># Create data
x1 &lt;- matrix(data = c(1,2,1.5))
y &lt;- matrix(data = c(2,1,1.75))
y_bar &lt;- mean(y)

# Plot data
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)

# Regress with sum of residuals
abline(a = y_bar,b = 0,col = &#39;red&#39;)

# Draw residuals
segments(x0 = 1,x1 = 1,y0 = 2, y1 = y_bar,col = &quot;blue&quot;,lwd = 3)
segments(x0 = 1.5,x1 = 1.5,y0 = 1.75, y1 = y_bar,col = &quot;blue&quot;,lwd = 3)
segments(x0 = 2,x1 = 2,y0 = 1, y1 = y_bar,col = &quot;blue&quot;,lwd = 3)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-6-1.png" width="672" />
Let’s do the numerical exercise.
The mean value of <span class="math inline">\(y =\)</span> 1.58 <span class="math inline">\(= \hat y\)</span></p>
<p>If we sum all these blue bars <span class="math inline">\(e_i\)</span> (where i indicates the residual of each point <span class="math inline">\(e_1\)</span> is the residual of the point (x=1,y=2), <span class="math inline">\(e_2\)</span> the residual of (1.5,1.75) and <span class="math inline">\(e_3\)</span> the residual of (2,1)), the result would be <span class="math inline">\(e_1 = 2 - 1.58\)</span> , <span class="math inline">\(e_2 = 1.75 - 1.58\)</span> , <span class="math inline">\(e_3 = 1.75 - 1.58\)</span> If we sum the residuals then 0.42 + -0.58 + 0.17 = 0</p>
<p>Instead we can use two strategies, either sum the absolute values of the residuals <span class="math inline">\(\sum \mid y - \hat y \mid\)</span> or the residual sum the squares <span class="math inline">\(\sum (y - \hat y)^2\)</span> also denoted as <span class="math inline">\(\mid\mid\ y - \hat y \mid\mid^2\)</span> which is the norm of the residuals, more on this later. In a different post we will explore the pros and cons of each one of them. But for now just remember that square of the residuals will penalize values that are far away, that way the line will cross the cloud of points. For now on, we will focus in this second one.</p>
<p>As we saw, sometimes the best regression line will not fit perfectly every point. That means that the <span class="math inline">\(y\)</span> is not in the column space of <span class="math inline">\(x1\)</span> variable. That means that there isn´t any linear combination of <span class="math inline">\(x1\)</span> that predicts <span class="math inline">\(y\)</span> perfectly. What do we do instead? We predict <span class="math inline">\(\hat y\)</span> which is the closest projection of <span class="math inline">\(y\)</span> to the column space of <span class="math inline">\(x1\)</span> and <span class="math inline">\(x_0\)</span>.</p>
<p>I use here a value for <span class="math inline">\(x_0\)</span> because you can interpret the intercept as a variable that always takes the value 1 like in this plot:</p>
<pre class="r"><code>x0 &lt;- matrix(data = c(1,1))
y &lt;- matrix(data = c(2,1))
plot(x0,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><strong>Question 2:</strong> How do we minimize the residual sum of squares in regression?</p>
<p><strong>Solution 2</strong> We have to project the <span class="math inline">\(y\)</span> results on the column space of <span class="math inline">\(X\)</span></p>
<p>Let’s start more slowly.We basically we want to predict the following form:</p>
<p><span class="math display">\[\hat y = \beta_0x_0 + \sum_{i=1}^p{X_i \beta_i} \]</span>
Where <span class="math inline">\(\hat y\)</span> is the closest or equal to <span class="math inline">\(y\)</span>. We want a value <span class="math inline">\(\hat y\)</span> such that <span class="math inline">\((y - \hat y)^2\)</span> is as small as possible.</p>
<p>This means that an intercept <span class="math inline">\(\beta_0\)</span> + the product of all the dependent variables we have times their coefficient <span class="math inline">\(\beta_i\)</span> will produce the projection <span class="math inline">\(\hat y\)</span> closest to the actual y.</p>
<p>If we go back to our original example with two points. <span class="math inline">\(\beta_0\)</span> it’s the the intercept = 3, and we only have 1 variable to predict so p = 1, because we only have one variable we only calculate 1 coefficient <span class="math inline">\(\beta_1\)</span>, which we saw it was -1. We can calculate then for example when our variable <span class="math inline">\(x1\)</span> = 1 we see that our formula spits out <span class="math inline">\(\hat y = \beta_0 + {X_1 \beta_1} = 3 + 1 * -1 = 2\)</span>. So conviniently this choice of <span class="math inline">\(\beta_0 = 3\)</span> and <span class="math inline">\(\beta_1 = -1\)</span> help us predict a value <span class="math inline">\(\hat y\)</span> that it’s the same as the actual <span class="math inline">\(y\)</span>. But in this case it was easy to do because there was a line that predicted every point.
But how do we go about if there isn’t any combination of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that perfectly predicts <span class="math inline">\(y\)</span>? In this case, mathematically speaking, it means <span class="math inline">\(y\)</span> doesn’t lie in the column space of <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_1\)</span> and we need to find the value for each <span class="math inline">\(\beta\)</span> that give us the <span class="math inline">\(\hat y\)</span> closest to <span class="math inline">\(y\)</span></p>
<p><strong>Question 2.1</strong> What is the column space?</p>
<p><strong>Solution</strong> The column space is every linear combination of predictor variables <span class="math inline">\(x_0\)</span> and all the other <span class="math inline">\(x_i\)</span>. From now on when I refer to predictor variables I will show it as a big <span class="math inline">\(X\)</span>. Which can be represented as a matrix: <span class="math display">\[\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\]</span></p>
<p>The first column is the values of <span class="math inline">\(x_0\)</span> and the second the values of <span class="math inline">\(x_1\)</span>. If we had more columns we would add more values. Then the column space would be the resulting space that could be predicted if we chose every value of <span class="math inline">\(\beta\)</span>. Let’s check an example. Imagine we are in the original data set with two points. To visually understand column space let’s choose one value for <span class="math inline">\(\beta_0\)</span> like 3 and let’s pick a few values of <span class="math inline">\(\beta_1\)</span> to see the different lines</p>
<pre class="r"><code>x0 &lt;- matrix(data = c(1,1))
x1 &lt;- matrix(data = c(1,2))
y &lt;- matrix(data = c(2,1))
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)

vals &lt;- seq(-10,10,length.out = 110)
#vals &lt;- seq(-10^2,10^2,length.out = 220)
for( i in vals){
  abline(a = 3,b = i,col = &#39;red&#39;) # Line one
}</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>All the red lines and the space in between represent the column space of <span class="math inline">\(x_1\)</span> (if the origin was fixed = 3). Meaning all the vectors we could create by choosing different values of <span class="math inline">\(\beta_1\)</span>. We see that there’s at least one line that passes through both points <span class="math inline">\(y_1,y_2\)</span> that means that <span class="math inline">\(y\)</span> which can be seen as a matrix like before <span class="math inline">\(\begin{bmatrix}2 \\ 1\end{bmatrix}\)</span> it’s in the column space of <span class="math inline">\(X = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span> because there’s a combination of betas <span class="math inline">\(\begin{bmatrix}\beta_0 \\ \beta_1\end{bmatrix} = \begin{bmatrix}3 \\ -1\end{bmatrix}\)</span> that multiplied by <span class="math inline">\(X= \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span> will produce the same output <span class="math inline">\(\hat y = y = \begin{bmatrix}2 \\ 1\end{bmatrix}\)</span>. So we can draw this in matrix form like: <span class="math display">\[X \beta = y\]</span> where lower case indicates a matrix of one column (<em>a vector</em>) and upper case is a matrix of 2 or more columns. The numeric example would be <span class="math display">\[\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix} \begin{pmatrix}\ 3 \\ -1\end{pmatrix} = \begin{pmatrix}2 \\ 1\end{pmatrix}\]</span></p>
<p>Remember that matrix multiplication is always rows times columns so its equivalent to <span class="math inline">\(1*3 + 2*-1 = 2\)</span> and <span class="math inline">\(1*3 + 1*-1 = 1\)</span>
For more on Linear Algebra and Matrices I highly recommend the videos of <span class="citation">@3blue1brown</span> on matrix multiplication<a href="https://youtu.be/LyGKycYT2v0">https://youtu.be/XkY2DOUCWMU</a>. He explains visually how this works</p>
<p>So for this case we were lucky that <span class="math inline">\(y\)</span> was on the column space or plainly speaking in the plane <span class="math inline">\(X\beta\)</span> but what happens when we have something like this</p>
<pre class="r"><code>x0 &lt;- matrix(data = c(1,1,1))
x1 &lt;- matrix(data = c(1,2,1.5))
y &lt;- matrix(data = c(2,1,1.75))
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)
vals &lt;- seq(-10,10,length.out = 110)
#vals &lt;- seq(-10^2,10^2,length.out = 220)
for( i in vals){
  abline(a = 3,b = i,col = &#39;red&#39;) # Line one
}</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Here, even though we have several lines that pass close to all the points. There isn’t any line that passess through all the points at the same time. Which means there isn’t any combination of <span class="math inline">\(\beta\)</span> that will result in the vector <span class="math inline">\(y\)</span>. If you imagine the column space of <span class="math inline">\(X\)</span> as a flat sheet of possible vectors and <span class="math inline">\(y\)</span> as a vector that does not lie in the same sheet it could be troublesome to imagine the translation from the 2D graph. The reason is you have to imagine one vector made of 3 points that is pointing upward in that sheet</p>
<p>This 3D visualization might be hard to understand but it will help you transition from the previous graph.
The red plane is the column space of <span class="math inline">\(X\)</span> but the actual solution <span class="math inline">\(y\)</span> it’s in a different blue plane.
<img src="/post/2020-02-09-regression-from-scratch-and-simple_files/Column%20Space%20with%20example.PNG" alt="3D Column Space and Y vector" /></p>
<p>If we simplify this 3D view to a sketch which shows the geometrical interpretation, we can see how the <span class="math inline">\(y\)</span> vector is not in the column space X.
<img src="/post/2020-02-09-regression-from-scratch-and-simple_files/Column%20Space.png" alt="Column_Space_Geometrical Interpretation" /></p>
<p>Then what is the best <span class="math inline">\(\hat y\)</span> that minimizes the error? Well the shortest line you can draw from <span class="math inline">\(y\)</span> to the column space of <span class="math inline">\(X\)</span> it’s the perpendicular line from <span class="math inline">\(y\)</span> to the column space. The same that the shortest distance between two points it’s a straight line, the shortest distance between a point and a plane is the perpenticular line from that point to the plane. Now instead of having a coefficient <span class="math inline">\(\beta\)</span> to calculate <span class="math inline">\(y\)</span> instead we have coefficient <span class="math inline">\(\hat \beta\)</span> to calculate <span class="math inline">\(\hat y\)</span> like <span class="math inline">\(\hat y = X \hat \beta\)</span></p>
<p>But you are probably like. Wait wait wait, in this tutorial we were going really slow through the explanation and all of a sudden I can’t connect the dots between this picture</p>
<pre class="r"><code>x1 &lt;- matrix(data = c(1,2,1.5))
y &lt;- matrix(data = c(2,1,1.75))
plot(x1,y,pch = 20, xlim = c(0,4), ylim = c(0,4),cex = 2)
abline(a = 3,b = -1,col = &#39;red&#39;)
segments(x0 = 1.5,x1 = 1.5,y0 = 1.75, y1 = 1.5,col = &quot;blue&quot;,lwd = 3)
text(1.6,1.6,&quot;Residual &#39;e_2&#39;&quot;,pos = 4)</code></pre>
<p><img src="/2020-02-09-regression-from-scratch-and-simple_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>and the geometrical interpretation.</p>
<p>Think about it this way, if there´s a plane that fits perfectly the 3 points in <span class="math inline">\(y\)</span> but it´s impossible to create with the <span class="math inline">\(X\)</span> variables, then the best regression line (that minimizes the magintude of the residuals) in the <span class="math inline">\(X\)</span> column space is the perpendicular projection of the vector <span class="math inline">\(y\)</span> on the <span class="math inline">\(X\)</span> column space plane. That is the essence of Least Squares. The line that minimizes the residual sum of squares is the orthogonal (otherwise perpendicular) projection of <span class="math inline">\(y\)</span> on the column space <span class="math inline">\(X\)</span></p>
<div class="figure">
<img src="/post/2020-02-09-regression-from-scratch-and-simple_files/smallest%20residual.png" alt="Smallest Residual vector is the one Perpendicular to the column space" />
<p class="caption">Smallest Residual vector is the one Perpendicular to the column space</p>
</div>
<p>Question 3: How do we calculate the orthogonal projection?</p>
<p>Solution 3: The orthogonal projection is equivalent to the inner product of the column space <span class="math inline">\(X\)</span> and the error.</p>
<p>Let’s break this down slowly. If we project the vector of the residual <span class="math inline">\(e\)</span> on to the plane the result will be 0. Because the vector of <span class="math inline">\(e\)</span> spans perpendicular to the column space <span class="math inline">\(X\)</span> plane. So the vector is 0. How do we do a perpendicular projection? In Euclidean spaces (our everyday spaces) with the dot product, that is equivalent to the inner product. In this case we want to project the residual e on the column space, with the dot product.</p>
<p>This is represented as <span class="math inline">\(\langle X,e\rangle= 0\)</span> which is the equivalent of multiplying every item of <span class="math inline">\(X\)</span> with the item of <span class="math inline">\(e\)</span> and adding all together. Another way of doing this is by transposing the <span class="math inline">\(X\)</span> Matrix (flipping it sideways) and doing matrix multiplication as before. Let’s do this</p>
<p><span class="math display">\[\langle X,e\rangle = X^Te = \begin{pmatrix} x_{01} &amp; x_{11} \\ x_{02} &amp; x_{12} \end{pmatrix}^T \cdot \begin{pmatrix}\ e_0 \\ e_1\end{pmatrix} = \begin{pmatrix} x_{01} &amp; x_{02} \\ x_{11} &amp; x_{12} \end{pmatrix} \cdot \begin{pmatrix}\ e_0 \\ e_1\end{pmatrix} = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix} \cdot \begin{pmatrix}\ e_0 \\ e_1\end{pmatrix} = \begin{pmatrix}1*e_0 + 1*e_0 \\ 1*e_1 + 2*e_1\end{pmatrix}\]</span>
Take time to do this yourself and understand what is the Transpose doing and why is that equivalent to the dot product.
To know more about the dot product see this video from <span class="citation">@3blue1brown</span> on <a href="https://youtu.be/LyGKycYT2v0">dot product</a>.</p>
<p>Next, we know that <span class="math inline">\(X^Te = 0\)</span> and we know that <span class="math inline">\(e = y - \hat y\)</span> moreover <span class="math inline">\(\hat y = X\hat\beta\)</span> putting it all together we have that</p>
<p><span class="math display">\[X^T(y - X\hat\beta) = 0\]</span>
With a little algebra we can solve this equation like this</p>
<p><span class="math display">\[X^Ty - X^TX\hat\beta = 0\\ X^Ty = X^TX\hat\beta \\  X^Ty \div X^TX = (X^TX)^{-1} X^Ty = \hat \beta \]</span></p>
<p>This <span class="math inline">\(\hat\beta = (X^TX)^{-1} X^Ty\)</span> is the most important formula to learn. And because we are using matrix notation it’s valid for 1 variable or 100 variables.</p>
<p>We can do one more trick. If <span class="math inline">\(\hat y = X \hat \beta\)</span> then <span class="math inline">\(\hat y = X (X^TX)^{-1} X^Ty\)</span>. This piece <span class="math inline">\(X (X^TX)^{-1} X^T = H\)</span> it’s called Hat Matrix because it puts the hat on the <span class="math inline">\(y\)</span>. <span class="math inline">\(\hat y = Hy\)</span> and it’s going to tell us a lot of things about the projection <span class="math inline">\(\hat y\)</span></p>
</div>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="/forcing-regression-coefficients-in-r-part-i/" title="Forcing Regression Coefficients in R - Part I">
      Previous
    </a>
    

    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  It&rsquo;s hard but you gotta keep trying


</div>


  
<div class="container copyright">
  
  (c) 2020 Alvaro Aguado


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

