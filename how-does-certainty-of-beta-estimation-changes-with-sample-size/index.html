<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Tutorial:How does certainty of beta estimation changes with sample size?  &middot; rTales: DataScience in Short</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="Easy, Regression, Least Squares, ">


<meta property="og:title" content="Tutorial:How does certainty of beta estimation changes with sample size?  &middot; rTales: DataScience in Short ">
<meta property="og:site_name" content="rTales: DataScience in Short"/>
<meta property="og:url" content="/how-does-certainty-of-beta-estimation-changes-with-sample-size/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2020-02-12T00:00:00Z" />
<meta property="og:article:modified_time" content="2020-02-12T00:00:00Z" />

  
    
<meta property="og:article:tag" content="Easy">
    
<meta property="og:article:tag" content="Regression">
    
<meta property="og:article:tag" content="Least Squares">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@@neuronalnet" />
<meta name="twitter:creator" content="@@neuronalnet" />
<meta name="twitter:title" content="Tutorial:How does certainty of beta estimation changes with sample size?" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="/how-does-certainty-of-beta-estimation-changes-with-sample-size/" />
<meta name="twitter:domain" content="/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Tutorial:How does certainty of beta estimation changes with sample size?",
    "author": {
      "@type": "Person",
      "name": "Alvaro Aguado"
    },
    "datePublished": "2020-02-12",
    "description": "",
    "wordCount":  2391 
  }
</script>



<link rel="canonical" href="/how-does-certainty-of-beta-estimation-changes-with-sample-size/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
<link href="/favicon.png" rel="icon" type="image/x-icon" />

<meta name="generator" content="Hugo 0.64.0" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-145400836-1', 'auto');
	  ga('send', 'pageview');

	</script>

</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <img src="/rTales-logo.png">
  <a class="baselink" href="/">
  

</a>

</div>

  
<div class="container topline">
  
  Solve practical problems with Data Science and Technology


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="/">Home</a>


  
<a href="/about">About</a>

<a href="/post" title="Show list of posts">Posts</a>

<a href="/resources">Resources</a>

<a href="/tags" title="Show list of tags">Tags</a>


</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" rel="me" aria-label="Email" href="mailto:alvaroaguado3@gmail.com">
  <span class="fa fa-envelope-square"></span></a>



<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/alvaroaguado3">
  <span class="fa fa-github-square"></span></a>




 
<a id="contact-link-linkedin" class="contact_link" rel="me" aria-label="LinkedIn" href="https://www.linkedin.com/in/alvaro-aguado-74314120/">
  <span class="fa fa-linkedin-square"></span></a>







<a id="contact-link-twitter" class="contact_link" rel="me" aria-label="Twitter" href="https://twitter.com/@neuronalnet">
  <span class="fa fa-twitter-square"></span></a>













</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Tutorial:How does certainty of beta estimation changes with sample size?
</h1>

  <div class="metas">
<time datetime="2020-02-12">12 Feb, 2020</time>


  
    &middot; by Alvaro Aguado
  
  &middot; Read in about 12 min
  &middot; (2391 Words)
  <br>
  
<a class="label" href="/tags/easy">Easy</a>

<a class="label" href="/tags/regression">Regression</a>

<a class="label" href="/tags/least-squares">Least Squares</a>


</div>

</header>

  <div class="container content">
  


<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<p>We first simulate data to see how to see how to calculate beta (slope) and alpha (constant) coefficients. We use a sampling method from a multivariate normal distribution with mean [0,0] and some covariance between independent variable x and target variable y.</p>
<pre class="python"><code># Import Libraries 
import numpy as np
import matplotlib.pyplot as plt

m = range(0,10000) # Datasets
n = 100 # number of points
sigx = .3 # Standard deviation x
sigy = .9 # Standard deviation y
rho = .1 # Pearson rho correlation used to show level of linear dependence

# generate data 
mu = np.array([0.,0.]) 
cov = np.array([[sigx*sigx,rho*sigx*sigy],[rho*sigx*sigy,sigy*sigy]]) 

data = np.random.multivariate_normal(mu,cov,n) # Sample data 

x = data[:,0]
y = data[:,1]

# fit the model 
covmat = np.cov(x,y) # covariance matrix
# cov of x,y is also estimated as rho(x,y)*sigma(x)*sigma(y)

# best fit parameters 
betahat = covmat[0,1]/covmat[0,0] # covariance x,y divided by variance x
alphahat = np.mean(y) - betahat*np.mean(x) 

# plot best fit model 
xvals = np.linspace(-1,1,150)
yvals = betahat*xvals + alphahat  

# Plot distribution density map
plt.figure()
plt.hist2d(x,y)</code></pre>
<pre><code>## (array([[0., 1., 1., 0., 0., 0., 0., 1., 0., 0.],
##        [0., 1., 0., 0., 1., 1., 0., 1., 0., 0.],
##        [1., 1., 1., 1., 1., 2., 2., 1., 0., 0.],
##        [0., 4., 2., 3., 6., 2., 2., 1., 1., 1.],
##        [1., 1., 2., 5., 2., 4., 1., 1., 0., 2.],
##        [2., 0., 2., 1., 3., 2., 0., 4., 2., 0.],
##        [0., 1., 0., 2., 2., 2., 3., 0., 1., 2.],
##        [0., 0., 3., 0., 1., 3., 0., 0., 0., 0.],
##        [0., 0., 0., 0., 1., 1., 0., 1., 0., 0.],
##        [0., 0., 0., 0., 1., 1., 0., 1., 0., 0.]]), array([-0.79008676, -0.62796122, -0.46583569, -0.30371015, -0.14158461,
##         0.02054092,  0.18266646,  0.344792  ,  0.50691753,  0.66904307,
##         0.83116861]), array([-1.84329895, -1.47780681, -1.11231466, -0.74682251, -0.38133036,
##        -0.01583822,  0.34965393,  0.71514608,  1.08063822,  1.44613037,
##         1.81162252]), &lt;matplotlib.collections.QuadMesh object at 0x000000002E3DA780&gt;)</code></pre>
<pre class="python"><code>plt.show()

# Plot data points + Regression</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="python"><code>plt.figure()
plt.plot(x,y,&#39;bx&#39;)
plt.plot(xvals,yvals,&#39;r-&#39;)
plt.show()
</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<div id="problem-1" class="section level3">
<h3>Problem 1:</h3>
<p>Plot histogram of these values and comment on its shape.
How does this histogram change as you change n and rho? Interpret the results</p>
<p>Letâ€™s first define a function that takes the innitial inputs and calculates beta several (m) times. We then store this list of betas to analyze the effect of changing n and rho.</p>
<pre class="python"><code>def betas(sigx,sigy,rho,n,m):
  &quot;&quot;&quot;function to calculate beta m times by sampling from normal distribution. 
  Function takes boolean inputs, returns list&quot;&quot;&quot;
  betas = [] # Store results
  mu = np.array([0.,0.])
  cov = np.array([[ sigx*sigx , rho*sigx*sigy ],[ rho*sigx*sigy , sigy*sigy]])
  for i in m:
    # LOOP over
    # Sample the data with mu and covariance matrix
    data = np.random.multivariate_normal(mu,cov,n)
    x = data[:,0]
    y = data[:,1]
    # fit the model
    covmat = np.cov(x,y) # covariance matrix of x and y
    betahat = covmat[0,1]/covmat[0,0] # Covmat[0,1] is the first row of the matrix covmat, 2nd column. And its the covariance of x,y, covmat[0,0] is the quasivariance of x
    alphahat = np.mean(y) - betahat*np.mean(x) #alphahat is the mean value of y - meanvalue of x times the slope. 
    # When x,y relationship is 1 to 1 then constant is the distance between average y and average x (scaling both axis to common origin, the constant)
    # Save beta in vector
    betas.append(betahat)
    # plot best fit model
    xvals = np.linspace(-1,1,150) # line of points where to calculate values
    yvals = betahat*xvals + alphahat
    
  return(betas)</code></pre>
<p>Now that we have the function letâ€™s visualize what happens when we use large n vs small n sample size.</p>
<pre class="python"><code># small n vs big n
n = 50
small_n = betas(sigx,sigy,rho,n,m)

n = 1000
big_n = betas(sigx,sigy,rho,n,m)

# Plot comparison for a small sample n vs big sample n
plt.figure()
plt.hist(small_n, bins = 30, color = &#39;blue&#39;)</code></pre>
<pre><code>## (array([1.000e+00, 0.000e+00, 1.000e+00, 4.000e+00, 5.000e+00, 1.600e+01,
##        3.000e+01, 5.200e+01, 8.700e+01, 2.100e+02, 3.360e+02, 4.800e+02,
##        6.560e+02, 8.820e+02, 1.011e+03, 1.114e+03, 1.126e+03, 1.108e+03,
##        9.330e+02, 7.050e+02, 4.920e+02, 3.500e+02, 1.820e+02, 1.110e+02,
##        6.600e+01, 2.000e+01, 1.900e+01, 1.000e+00, 1.000e+00, 1.000e+00]), array([-1.73054041, -1.60360073, -1.47666106, -1.34972138, -1.2227817 ,
##        -1.09584203, -0.96890235, -0.84196268, -0.715023  , -0.58808332,
##        -0.46114365, -0.33420397, -0.2072643 , -0.08032462,  0.04661506,
##         0.17355473,  0.30049441,  0.42743408,  0.55437376,  0.68131344,
##         0.80825311,  0.93519279,  1.06213246,  1.18907214,  1.31601182,
##         1.44295149,  1.56989117,  1.69683084,  1.82377052,  1.9507102 ,
##         2.07764987]), &lt;a list of 30 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.hist(big_n, bins = 30, color = &#39;red&#39;)</code></pre>
<pre><code>## (array([3.000e+00, 3.000e+00, 4.000e+00, 1.100e+01, 2.000e+01, 5.700e+01,
##        8.700e+01, 1.420e+02, 2.140e+02, 3.310e+02, 4.520e+02, 6.220e+02,
##        7.600e+02, 9.300e+02, 1.006e+03, 1.006e+03, 9.560e+02, 8.450e+02,
##        7.390e+02, 6.020e+02, 4.250e+02, 2.900e+02, 2.140e+02, 1.270e+02,
##        8.400e+01, 4.000e+01, 1.900e+01, 8.000e+00, 1.000e+00, 2.000e+00]), array([-0.06804927, -0.04414042, -0.02023157,  0.00367728,  0.02758613,
##         0.05149498,  0.07540383,  0.09931268,  0.12322153,  0.14713038,
##         0.17103923,  0.19494808,  0.21885693,  0.24276578,  0.26667463,
##         0.29058348,  0.31449234,  0.33840119,  0.36231004,  0.38621889,
##         0.41012774,  0.43403659,  0.45794544,  0.48185429,  0.50576314,
##         0.52967199,  0.55358084,  0.57748969,  0.60139854,  0.62530739,
##         0.64921624]), &lt;a list of 30 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In blue we see how a small n makes the uncertainty around the beta greater. It makes the number oscillate between 1 and 3.
While when our sampling size is greater our potential betas are more concentrated around the mean.
See the differences between the means and the standard deviations</p>
<pre class="python"><code># print means for both groups
np.mean(small_n)</code></pre>
<pre><code>## 0.30678686467002664</code></pre>
<pre class="python"><code>np.mean(big_n)

# print Std for both groups </code></pre>
<pre><code>## 0.2996550745697297</code></pre>
<pre class="python"><code>np.std(small_n)</code></pre>
<pre><code>## 0.4398458689400365</code></pre>
<pre class="python"><code>np.std(big_n)</code></pre>
<pre><code>## 0.09479983762513984</code></pre>
<p>Now letâ€™s do the same for a large value of rho and and a small effect closer to 0.</p>
<pre class="python"><code>
# Small Rho vs big Rho
n = 100 # adjust again n 

# Small Rho
rho = 0.01
small_rho = betas(sigx,sigy,rho,n,m)

# Big Rho
rho = 0.9
big_rho = betas(sigx,sigy,rho,n,m)

# Plot comparison for a small rho n vs big rho
plt.figure()
plt.hist(small_rho, bins = 30, color = &#39;blue&#39;)</code></pre>
<pre><code>## (array([  1.,   4.,  10.,  16.,  37.,  61., 111., 199., 271., 399., 546.,
##        626., 780., 900., 952., 955., 860., 834., 686., 598., 388., 272.,
##        195., 107.,  81.,  54.,  28.,  19.,   8.,   2.]), array([-1.05440405, -0.98281469, -0.91122533, -0.83963596, -0.7680466 ,
##        -0.69645724, -0.62486787, -0.55327851, -0.48168915, -0.41009978,
##        -0.33851042, -0.26692106, -0.19533169, -0.12374233, -0.05215297,
##         0.0194364 ,  0.09102576,  0.16261512,  0.23420449,  0.30579385,
##         0.37738321,  0.44897258,  0.52056194,  0.5921513 ,  0.66374067,
##         0.73533003,  0.80691939,  0.87850876,  0.95009812,  1.02168748,
##         1.09327685]), &lt;a list of 30 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.hist(big_rho, bins = 30, color = &#39;red&#39;)</code></pre>
<pre><code>## (array([  3.,   5.,   1.,  12.,  22.,  43.,  66., 133., 206., 336., 456.,
##        574., 682., 836., 996., 979., 980., 893., 730., 613., 501., 346.,
##        248., 137., 101.,  50.,  26.,  14.,   5.,   6.]), array([2.18535395, 2.21823445, 2.25111495, 2.28399545, 2.31687595,
##        2.34975645, 2.38263695, 2.41551745, 2.44839795, 2.48127845,
##        2.51415895, 2.54703945, 2.57991995, 2.61280045, 2.64568095,
##        2.67856145, 2.71144195, 2.74432245, 2.77720295, 2.81008345,
##        2.84296395, 2.87584445, 2.90872495, 2.94160545, 2.97448595,
##        3.00736645, 3.04024695, 3.07312745, 3.10600795, 3.13888845,
##        3.17176895]), &lt;a list of 30 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>In this case we see how rho affects at the correlation between x and y. When rho is small the values of beta are more likely to be zero.
On the other hand, when rho is high values of beta are extremely different from 0</p>
</div>
<div id="problem-2" class="section level3">
<h3>Problem 2:</h3>
<p>Implement the standard error formula for betahat. Extend your work in the prior problem to compute the normalized coefficient zhat = betahat/std(betahat). Look into scipy.stats package and find the student-t distribution. Use the t.pdf() function to plot a student-t distribution with v = n - 2 degrees of freedom on top of a histogram of zhat values and compare the two.</p>
<p>For this we will first implement a standard error formula for betahat. This function is defined as we saw in class as</p>
<pre class="python"><code># Define the Standard Error formula
def se_beta(y,x,df):
  # An estimate for se(betahat) = sqrt((1/v sum(yhat - yi)^2)/sum(xi-mean(x)^2))
  
  # define degrees of freedom t distribution
  v = 1/df
  
  # define y hat from the beta value
  yhat = alphahat + betahat*x # estimate yhat
  # Calculate standard error of betahat
  # Calculate numerator and denominator
  num = v * np.sum((yhat - y)**2) # sum of squared error of predicted yhat
  dem = np.sum((x - np.mean(x))**2) # Var of x
  
  # return se_beta: how much does yhat err scaled by the variance of x
  return(np.sqrt(num/dem))


# Include this in the function we created previously and store zhat of every iteration

def zetas(sigx,sigy,rho,n,m):
  &quot;&quot;&quot;docstring for betas&quot;&quot;&quot;
  zetas = [] # Store results
  mu = np.array([0.,0.])
  cov = np.array([[ sigx*sigx , rho*sigx*sigy ],[ rho*sigx*sigy , sigy*sigy]])
  for i in m:
    # LOOP over
    # Sample the data with mu and covariance matrix
    data = np.random.multivariate_normal(mu,cov,n)
    x = data[:,0]
    y = data[:,1]
    
    # fit the model
    covmat = np.cov(x,y) # covariance matrix of x and y
    betahat = covmat[0,1]/covmat[0,0] # Covmat[0,1] is the first row of the matrix covmat, 2nd column. And its the covariance of x,y, covmat[0,0] is the quasivariance of x
    alphahat = np.mean(y) - betahat*np.mean(x) #alphahat is the mean value of y - meanvalue of x times the slope. 
    
    # Calculate Standard Error and zhat
    se_betahat = se_beta(y,x,df = (n-2))
    zhat = (betahat / se_betahat)
    # Save zeta in vector
    zetas.append(zhat)
    
  return(zetas)


# Create t distribution line rv, and probability (pdf)
from scipy.stats import t
rv = t(df=(n - 2), loc=0, scale=1)
# plot best fit model
xvals = np.linspace(-4,4,150) # line of points where to calculate values
yvals = betahat*xvals + alphahat</code></pre>
<p>Now that we can calculate the standard error of beta we can calculate the z-score and compare that to the theoretical t-student distribution around value 0. We can visually see if one is different from the other.</p>
<pre class="python"><code># rho high 
rho = 0.7
zeta_hats = zetas(sigx,sigy,rho,n,m)

# Compare t-distribution with v degrees of freedom (blue) and actual zhat distribution when rho is high (orange)
plt.figure()
plt.plot(xvals,rv.pdf(x=xvals))
plt.hist(zeta_hats,density = True,histtype = &#39;stepfilled&#39;,bins = 50)</code></pre>
<pre><code>## (array([0.0019981 , 0.00099905, 0.        , 0.        , 0.0039962 ,
##        0.00099905, 0.00799241, 0.00699336, 0.00999051, 0.00699336,
##        0.01898197, 0.03196963, 0.03796394, 0.04595635, 0.05994306,
##        0.06893452, 0.08392029, 0.09990511, 0.1388681 , 0.15884912,
##        0.24177036, 0.26175138, 0.32069539, 0.38962991, 0.39962042,
##        0.47055305, 0.51950655, 0.5654629 , 0.57845056, 0.62940217,
##        0.63239932, 0.65337939, 0.61141925, 0.53349327, 0.48953502,
##        0.43358816, 0.37264605, 0.29971532, 0.24476751, 0.1898197 ,
##        0.13187474, 0.09291175, 0.06194117, 0.03996204, 0.01998102,
##        0.00699336, 0.00699336, 0.00499526, 0.00099905, 0.00099905]), array([4.62911914, 4.72921413, 4.82930911, 4.92940409, 5.02949908,
##        5.12959406, 5.22968905, 5.32978403, 5.42987901, 5.529974  ,
##        5.63006898, 5.73016397, 5.83025895, 5.93035394, 6.03044892,
##        6.1305439 , 6.23063889, 6.33073387, 6.43082886, 6.53092384,
##        6.63101882, 6.73111381, 6.83120879, 6.93130378, 7.03139876,
##        7.13149374, 7.23158873, 7.33168371, 7.4317787 , 7.53187368,
##        7.63196866, 7.73206365, 7.83215863, 7.93225362, 8.0323486 ,
##        8.13244358, 8.23253857, 8.33263355, 8.43272854, 8.53282352,
##        8.6329185 , 8.73301349, 8.83310847, 8.93320346, 9.03329844,
##        9.13339342, 9.23348841, 9.33358339, 9.43367838, 9.53377336,
##        9.63386834]), &lt;a list of 1 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.show()


# rho small (small beta)</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="python"><code>rho = 0.01
zeta_hats = zetas(sigx,sigy,rho,n,m)

# Compare t-distribution with v degrees of freedom (blue) and actual zhat distribution when rho is low (orange)
plt.figure()
plt.plot(xvals,rv.pdf(x=xvals))
plt.hist(zeta_hats,density = True,histtype = &#39;stepfilled&#39;,bins = 50)</code></pre>
<pre><code>## (array([0.00069071, 0.00138142, 0.        , 0.00345354, 0.00483495,
##        0.00552566, 0.00483495, 0.01726769, 0.02693759, 0.0366075 ,
##        0.06354509, 0.07183358, 0.08495703, 0.12018311, 0.16231627,
##        0.18925387, 0.22655207, 0.26108745, 0.3025299 , 0.32739537,
##        0.34742589, 0.38748693, 0.41028028, 0.4033732 , 0.4213316 ,
##        0.3937033 , 0.38196127, 0.34604448, 0.32117901, 0.29286   ,
##        0.23622198, 0.23553127, 0.17682113, 0.15817203, 0.13330656,
##        0.0918641 , 0.06768934, 0.05801943, 0.03522608, 0.02970042,
##        0.01726769, 0.02279335, 0.01312344, 0.00621637, 0.00207212,
##        0.00207212, 0.00069071, 0.00207212, 0.00069071, 0.00069071]), array([-3.36895644, -3.22417737, -3.07939829, -2.93461921, -2.78984014,
##        -2.64506106, -2.50028198, -2.35550291, -2.21072383, -2.06594475,
##        -1.92116568, -1.7763866 , -1.63160752, -1.48682845, -1.34204937,
##        -1.19727029, -1.05249122, -0.90771214, -0.76293306, -0.61815399,
##        -0.47337491, -0.32859583, -0.18381676, -0.03903768,  0.1057414 ,
##         0.25052047,  0.39529955,  0.54007863,  0.6848577 ,  0.82963678,
##         0.97441586,  1.11919493,  1.26397401,  1.40875309,  1.55353216,
##         1.69831124,  1.84309032,  1.98786939,  2.13264847,  2.27742755,
##         2.42220662,  2.5669857 ,  2.71176478,  2.85654385,  3.00132293,
##         3.14610201,  3.29088108,  3.43566016,  3.58043924,  3.72521831,
##         3.86999739]), &lt;a list of 1 Patch objects&gt;)</code></pre>
<pre class="python"><code>plt.show()
</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>When beta coefficient is high thereâ€™s a high probability the value is different than 0
empirical sampling shows little to no overlap when rho is high, basically discarding the hypothesis that beta = 0
However, overlap seems significant when rho is close to 0.</p>
</div>
<div id="problem-3" class="section level3">
<h3>Problem 3:</h3>
<p>Implement the p-value formula we mentioned in class using the quad method in scipy.integrate, and the student-t distribution from the prior problem
Next, consider the OLS method in statsmodels. Fit a dataset using both methods that we considered in class and statmodels methods.
Compute the summary stats of the statsmodels method, which includes betahat errors as well as associated p-values with your results from Problem 2</p>
<pre class="python"><code># P-value formula using scipy.integrate

import scipy.integrate as Int
rho = 0.3
mu = np.array([0.,0.])
cov = np.array([[ sigx*sigx , rho*sigx*sigy ],[ rho*sigx*sigy , sigy*sigy]])
data = np.random.multivariate_normal(mu,cov,n)
x = data[:,0]
y = data[:,1]

# fit the model
covmat = np.cov(x,y) # covariance matrix of x and y
betahat = covmat[0,1]/covmat[0,0] # Covmat[0,1] is the first row of the matrix covmat, 2nd column. And its the covariance of x,y, covmat[0,0] is the quasivariance of x
alphahat = np.mean(y) - betahat*np.mean(x) #alphahat is the mean value of y - meanvalue of x times the slope. 

# Calculate Standard Error and zhat
se_betahat = se_beta(y,x,df = (n-2))
zhat = (betahat / se_betahat)


pvalue_integration = Int.quad(rv.pdf,zhat,np.inf)[0]*2

# Check with OLS method
import statsmodels.api as sm
# help(sm.OLS)
X = sm.add_constant(x)
model = sm.OLS(y,X).fit()
model.summary()</code></pre>
<pre><code>## &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
## &quot;&quot;&quot;
##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                      y   R-squared:                       0.090
## Model:                            OLS   Adj. R-squared:                  0.081
## Method:                 Least Squares   F-statistic:                     9.719
## Date:                Wed, 12 Feb 2020   Prob (F-statistic):            0.00239
## Time:                        20:51:48   Log-Likelihood:                -123.22
## No. Observations:                 100   AIC:                             250.4
## Df Residuals:                      98   BIC:                             255.7
## Df Model:                           1                                         
## Covariance Type:            nonrobust                                         
## ==============================================================================
##                  coef    std err          t      P&gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## const         -0.0970      0.084     -1.155      0.251      -0.264       0.070
## x1             0.9513      0.305      3.117      0.002       0.346       1.557
## ==============================================================================
## Omnibus:                        0.267   Durbin-Watson:                   1.876
## Prob(Omnibus):                  0.875   Jarque-Bera (JB):                0.446
## Skew:                          -0.012   Prob(JB):                        0.800
## Kurtosis:                       2.674   Cond. No.                         3.64
## ==============================================================================
## 
## Warnings:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
## &quot;&quot;&quot;</code></pre>
<pre class="python"><code>pvalue_OLS = model.pvalues[1]

# plot distribution and z score
plt.figure()
plt.plot(xvals,rv.pdf(xvals))
plt.vlines(zhat,0,0.5)
plt.show()</code></pre>
<p><img src="/2020-02-12-how-variance-changes-in-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can visually see that the z-score value is far from the mean. Now we can calculate the probability that the value is different from zero. In this case we are going to compare the 2 methods we used.</p>
<pre class="python"><code># P-value via integration
print(&#39;-------- P-value via integration -----------&#39;)</code></pre>
<pre><code>## -------- P-value via integration -----------</code></pre>
<pre class="python"><code>print(pvalue_integration)
# P-value via statsmodels package </code></pre>
<pre><code>## 0.002394195894871735</code></pre>
<pre class="python"><code>print(&#39;-------- P-value via statsmodels package -----------&#39;)</code></pre>
<pre><code>## -------- P-value via statsmodels package -----------</code></pre>
<pre class="python"><code>print(pvalue_OLS)
</code></pre>
<pre><code>## 0.00239419589487163</code></pre>
<p>As we can see both p-values are exactly the same when we calculate them via integration or with the statsmodel package</p>
</div>
</div>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  

    
    <a class="prev" href="/regression-from-scratch-and-simple/" title="Tutorial:Regression from Scratch and Simple">
      Previous
    </a>
    

    

  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  It&rsquo;s hard but you gotta keep trying


</div>


  
<div class="container copyright">
  
  (c) 2020 Alvaro Aguado


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

