<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Forcing Regression Coefficients in R - Part I  &middot; rTales: DataScience in Short</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="regression, ">


<meta property="og:title" content="Forcing Regression Coefficients in R - Part I  &middot; rTales: DataScience in Short ">
<meta property="og:site_name" content="rTales: DataScience in Short"/>
<meta property="og:url" content="/2019/08/06/restricted-coefficients-linear-regression-part-i/" />
<meta property="og:locale" content="en-EN">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2019-08-06T00:00:00Z" />
<meta property="og:article:modified_time" content="2019-08-06T00:00:00Z" />

  
    
<meta property="og:article:tag" content="regression">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@https://twitter.com/neuronalnet" />
<meta name="twitter:creator" content="@https://twitter.com/neuronalnet" />
<meta name="twitter:title" content="Forcing Regression Coefficients in R - Part I" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="/2019/08/06/restricted-coefficients-linear-regression-part-i/" />
<meta name="twitter:domain" content="/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Forcing Regression Coefficients in R - Part I",
    "author": {
      "@type": "Person",
      "name": "Alvaro Aguado"
    },
    "datePublished": "2019-08-06",
    "description": "",
    "wordCount":  1636 
  }
</script>



<link rel="canonical" href="/2019/08/06/restricted-coefficients-linear-regression-part-i/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
<link href="/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.56.3" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link href='https://fonts.googleapis.com/css?family=Merriweather:300%7CRaleway%7COpen+Sans' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight/default.css">

  
  
</head>
<body>
  <main id="main-wrapper" class="container main_wrapper has-sidebar">
    <header id="main-header" class="container main_header">
  <div class="container brand">
  <div class="container title h1-like">
  <img src="rTales-logo.png">
  <a class="baselink" href="/">
  

</a>

</div>

  
<div class="container topline">
  
  Solve practical problems with Data Science and Technology


</div>


</div>

  <nav class="container nav primary no-print">
  

<a class="homelink" href="/">home</a>


  
<a href="/about">About</a>

<a href="/post" title="Show list of posts">Posts</a>

<a href="/tags" title="Show list of tags">Tags</a>


</nav>

<div class="container nav secondary no-print">
  
<a id="contact-link-email" class="contact_link" rel="me" aria-label="Email" href="mailto:alvaroaguado3@gmail.com">
  <span class="fa fa-envelope-square"></span></a>



<a id="contact-link-github" class="contact_link" rel="me" aria-label="Github" href="https://github.com/alvaroaguado3">
  <span class="fa fa-github-square"></span></a>




 
<a id="contact-link-linkedin" class="contact_link" rel="me" aria-label="LinkedIn" href="https://www.linkedin.com/in/https://www.linkedin.com/in/alvaro-aguado-74314120/">
  <span class="fa fa-linkedin-square"></span></a>







<a id="contact-link-twitter" class="contact_link" rel="me" aria-label="Twitter" href="https://twitter.com/https://twitter.com/neuronalnet">
  <span class="fa fa-twitter-square"></span></a>













</div>


  

</header>


<article id="main-content" class="container main_content single">
  <header class="container hat">
  <h1>Forcing Regression Coefficients in R - Part I
</h1>

  <div class="metas">
<time datetime="2019-08-06">6 Aug, 2019</time>


  
    &middot; by Alvaro Aguado
  
  &middot; Read in about 8 min
  &middot; (1636 Words)
  <br>
  
<a class="label" href="/tags/regression">regression</a>



</div>

</header>

  <div class="container content">
  


<div id="question" class="section level3">
<h3>Question:</h3>
<p>I’m working on a regression model and I need to force the regression coefficients to be positive. How can I accomplish this in R?</p>
</div>
<div id="solution" class="section level3">
<h3>Solution:</h3>
<p>For linear model regression model with restricted coefficients you have 3 options: Linear with nls, Bayes with brms and Lasso. Here we will look at <code>Linear Model with nls</code></p>
<p>The function lm does not provide a way to restrict coefficients. Instead we can use the function nls under the algorithm port. This allow us to provide start for the coefficients, as well as upper and lower boundaries.</p>
<p>In order to define the coefficients create arbitrary coefficient names and define them just like I did below. I picked “a” to be my intercept and “b1”,“b2” and “b3” to be my coefficients for the individual variables</p>
<pre class="r"><code># Load libraries
require(tidyverse) #manipulate data
require(knitr) # See Table

# Use formula nls instead of lm and select algorithm port
fit1 &lt;- nls(formula =  Sepal.Length ~ a + b1*Sepal.Width +  b2*Petal.Length + b3*Petal.Width,
    data = iris, 
    start = list(a = 0,b1 = 0.0,b2 = 0, b3 = 0),
    lower = c(a = 0,b1 = 0,b2 = 0, b3 = 0),
    upper = c(a = Inf,b1 = Inf,b2 = Inf, b3 = Inf),
    algorithm = &quot;port&quot;) 

# We see that the coefficients are equal or greater than 0 
c.fit1 &lt;- data.frame(t(coef(fit1))) 
colnames(c.fit1) &lt;- c(&quot;(Intercept)&quot;,colnames(iris)[2:4])
knitr::kable(c.fit1, caption = &quot;Forced Positive Coefficients&quot;)</code></pre>
<table>
<caption>(#tab:Linear_model)Forced Positive Coefficients</caption>
<thead>
<tr class="header">
<th align="right">(Intercept)</th>
<th align="right">Sepal.Width</th>
<th align="right">Petal.Length</th>
<th align="right">Petal.Width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2.24914</td>
<td align="right">0.5955247</td>
<td align="right">0.47192</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<ul>
<li>For Solution with <a href="">Bayes</a></li>
<li>For Solution with <a href="">Lasso</a></li>
</ul>
</div>
<div id="business-applications" class="section level3">
<h3>Business Applications:</h3>
<p><code>Marketing Mix</code>,<code>Price Elasticity</code></p>
<p>In several situations, like building a marketing mix model, I found myself with variables that have to be included in a model but the coefficient ended up with the opposite sign as wanted. It’s very important in these cases to address if there are any other data issues before forcing a coefficient, since this will not make the model optimal from an OLS (Ordinary Least Squares) perspective.</p>
<p>For example, I had cases in which Marketing Tactics such as TV was providing negative effect in sales. While this could be true, it is uncommon to see TV ads that have negative effect in the market. A good remedy is to provide positive priors and regress using bayes approach. That could help the algorithm find I new local minima. You can find such positive priors from correlations between TV and Sales using past data prior to the model, or manually providing industry averages.</p>
<p>Another case is when calculating Price Elasticity. We might find that, as price increases so does sales volume. And again this could be again true if there’s scarcity of the product, or if higher price makes the product look more attractive to consumers. Again, I found myself in the situation where price elasticity did not make sense from a business perspective.</p>
<p>However, this was because I had relatively low number of data points to make my estimation. When prices are very concentrated in a small range, it is very hard to find the actual slope of the regression. To solve this, I first recommend to ask for more data, and if this is not possible and it’s absolutely necessary to provide an answer then I turn to ‘Shrinkage methods’ like lasso.</p>
</div>
<div id="detailed-example" class="section level3">
<h3>Detailed Example</h3>
<pre class="r"><code># Libraries
require(tidyverse) #Manipulate data
require(DT)        #Visualize data in tables

# Function to visuzalize regression line on plot
reg &lt;- function(x, y, col) abline(lm(y ~ x), col = col) 

# Aestethics for the Pairs Plot 
panel.lm =  function(x, y, col = par(&quot;col&quot;), bg = NA, pch = par(&quot;pch&quot;), 
    cex = 1, col.smooth = &quot;red&quot;, span = 2/3, iter = 3, ...)  {
    points(x, y, pch = pch, col = col, bg = bg, cex = cex)
    ok &lt;- is.finite(x) &amp; is.finite(y)
    if (any(ok)) reg(x[ok], y[ok], col.smooth)
}

# Other Aesthetics, labels and others
panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...)
{
 usr &lt;- par(&quot;usr&quot;); on.exit(par(usr))
 par(usr = c(0, 1, 0, 1))
 r &lt;- abs(cor(x, y))
 txt &lt;- format(c(r, 0.123456789), digits = digits)[1]
 txt &lt;- paste0(prefix, txt)
 text(0.5, 0.5, txt, cex = 1.1, font = 4)
}

# Variable MPG and Disp show negative correlation
pairs(mtcars[c(1,3)], panel = panel.lm,
    cex = 1.5, pch = 19, col = adjustcolor(4, .4), cex.labels = 2, 
    font.labels = 2, lower.panel = panel.cor)</code></pre>
<p><img src="/post/2019-08-06-restricted-coefficients-linear-regression-part-i_files/figure-html/Problem_to_Solve-1.png" width="672" /></p>
<pre class="r"><code># However when we regress all variables disp has positive slope
lm(mpg ~ ., data = mtcars) %&gt;% 
  broom::tidy() %&gt;% 
  select(1,2) %&gt;% 
  mutate(estimate = round(estimate,4)) %&gt;% 
  knitr::kable(caption = &quot;OLS Regression Coefficients for dependent variable MPG&quot;)</code></pre>
<table>
<caption>(#tab:Regression_coef_Incorrect)OLS Regression Coefficients for dependent variable MPG</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">12.3034</td>
</tr>
<tr class="even">
<td align="left">cyl</td>
<td align="right">-0.1114</td>
</tr>
<tr class="odd">
<td align="left">disp</td>
<td align="right">0.0133</td>
</tr>
<tr class="even">
<td align="left">hp</td>
<td align="right">-0.0215</td>
</tr>
<tr class="odd">
<td align="left">drat</td>
<td align="right">0.7871</td>
</tr>
<tr class="even">
<td align="left">wt</td>
<td align="right">-3.7153</td>
</tr>
<tr class="odd">
<td align="left">qsec</td>
<td align="right">0.8210</td>
</tr>
<tr class="even">
<td align="left">vs</td>
<td align="right">0.3178</td>
</tr>
<tr class="odd">
<td align="left">am</td>
<td align="right">2.5202</td>
</tr>
<tr class="even">
<td align="left">gear</td>
<td align="right">0.6554</td>
</tr>
<tr class="odd">
<td align="left">carb</td>
<td align="right">-0.1994</td>
</tr>
</tbody>
</table>
<p>Now let’s force the coefficients to be positive for disp</p>
<pre class="r"><code># Create formula for all coefficients
b_part &lt;- paste0(&quot;b&quot;,1:(ncol(mtcars) - 1),&quot;*&quot;, colnames(mtcars)[-1],
                 collapse = &quot; + &quot;)
formula &lt;- paste0(colnames(mtcars)[1],&quot; ~ a + &quot;, b_part)

# Create List for the restrictions
ab_list &lt;- c(&quot;a&quot;,paste0(&quot;b&quot;,1:(ncol(mtcars) - 1)))
start &lt;- rep(list(0),ncol(mtcars))
names(start) &lt;- ab_list
lower &lt;- rep(list(0),ncol(mtcars))
names(lower) &lt;- ab_list
upper &lt;- rep(list(Inf),ncol(mtcars))
names(upper) &lt;- ab_list

# Run the linear Regression with all the restrictions 
nls(formula =  formula, 
    data = mtcars,
    start = start,
    lower = lower,
    upper = upper,
    algorithm = &quot;port&quot;) -&gt; fit3 
  fit3 %&gt;%  
  broom::tidy() %&gt;% 
  select(1,2) %&gt;% 
  mutate(term = c(&quot;(Intercept)&quot;,colnames(mtcars)[-1]),
         estimate_restricted = estimate) %&gt;% 
  select(term,estimate_restricted) -&gt; tidy_fit3

lm(mpg ~ ., data = mtcars) -&gt; fit4
fit4 %&gt;% 
  broom::tidy() %&gt;% 
  select(1,2) %&gt;% 
  mutate(estimate_original = estimate) %&gt;% 
  select(term,estimate_original) %&gt;% 
  inner_join(tidy_fit3, by = &quot;term&quot;) -&gt; tidy_fit3

knitr::kable(tidy_fit3, caption = &quot;Linear Regression Coefficients for dependent variable MPG: OLS vs. Forced Coefficient&quot;)</code></pre>
<table>
<caption>(#tab:Regression_coef_forced)Linear Regression Coefficients for dependent variable MPG: OLS vs. Forced Coefficient</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate_original</th>
<th align="right">estimate_restricted</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">12.3033742</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">cyl</td>
<td align="right">-0.1114405</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">disp</td>
<td align="right">0.0133352</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">hp</td>
<td align="right">-0.0214821</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">drat</td>
<td align="right">0.7871110</td>
<td align="right">1.2139178</td>
</tr>
<tr class="even">
<td align="left">wt</td>
<td align="right">-3.7153039</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">qsec</td>
<td align="right">0.8210407</td>
<td align="right">0.6317301</td>
</tr>
<tr class="even">
<td align="left">vs</td>
<td align="right">0.3177628</td>
<td align="right">4.6582898</td>
</tr>
<tr class="odd">
<td align="left">am</td>
<td align="right">2.5202269</td>
<td align="right">6.0007956</td>
</tr>
<tr class="even">
<td align="left">gear</td>
<td align="right">0.6554130</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">carb</td>
<td align="right">-0.1994193</td>
<td align="right">0.0000000</td>
</tr>
</tbody>
</table>
<p>So now the question is how much worse is this new model.</p>
<p>If we compare residuals for each model original OLS regression had 12.1447 whereas the restricted model 17.6378.</p>
<p>So clearly the new model is worse. We must keep in mind that this is an extreme case, since we have forced everything to be positive. We should carefully pick what variables should be restricted and which ones shouldn’t.</p>
<p>It’s up to the analyst to consider whether this model is ‘good enough’ to be used for business purposes. What do I mean by good enough? the applications of a regression model are uncountable, and we must consider what is the objective. For instance, we could be trying to assess if there’s any positive effect of variable x on y when we incorporate all the other variables. In that case, even if the model is worse, we would keep the results of the model.</p>
<p>In this case, we are going to assess if the differences are statistically and also visually looking at the predictions.</p>
<p>To test if both models are statistically different we can run an ANOVA test.</p>
<pre class="r"><code># Because nls does not behave well with Anova I create a lm model with forced coefficients. You can better understand this concept [here]()

# Compare Complex model to more simple model
formula2 &lt;- paste0(&quot;mpg ~ 0 +&quot; , paste0(&quot;offset(&quot;,coef(fit3)[-1],&quot;*&quot;,colnames(mtcars)[-1],&quot;)&quot;,collapse = &quot; + &quot;))
lm(formula2 , data = mtcars) %&gt;% anova(.,fit4) %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 x 6
##   res.df   rss    df sumsq statistic p.value
## *  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1     32  311.    NA   NA      NA    NA     
## 2     21  147.    11  164.      2.12  0.0671</code></pre>
<p>Results from Anova show that both models are not significantly different since p-value &gt; 0.05. I always consider this threshold a little arbitrary, and the analyst should personally estimate whether this difference is significant or not, and validate results visually, as we are going to do below</p>
<p>Let’s take a look at how each model predicts the output. Let’s keep in mind that we are predicting the response variable using the train set. This is, that all the cases that are used to train the model are the same we are using to compare accuracy. In this case we are going to omit this fact, but it is recommendable to keep a hold out set to compare both models fairly.</p>
<pre class="r"><code>require(ggplot)
require(cowplot) 
predict(fit3) -&gt; pfit3
predict(fit4) -&gt; pfit4

cols = c(&quot;Restricted Model (nls)&quot; = &quot;firebrick&quot;,
         &quot;OLS Model (lm)&quot; = &quot;blue&quot;)
ggplot(mtcars,aes(x = 1:length(mpg),y = mpg)) + 
  geom_point() +
  geom_line(aes(y = pfit3,colour = &quot;Restricted Model (nls)&quot;),size = 1) +
  geom_line(aes(y = pfit4,colour = &quot;OLS Model (lm)&quot;),size = 1) +
  scale_colour_manual(name=&quot;Regression Lines&quot;,values = cols) +
  scale_x_continuous(name = &quot;Car Models&quot;,
                     breaks = 1:nrow(mtcars),
                     labels = row.names(mtcars)) +
    ggtitle(label = &quot;Estimated mpg by car model&quot;,
            subtitle = &quot;Regular OLS method vs. Restricted positive coefficients&quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,size = 8),
        legend.position = &quot;top&quot;,
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        plot.title = element_text(size = 12, face = &quot;bold&quot;)) </code></pre>
<p><img src="/post/2019-08-06-restricted-coefficients-linear-regression-part-i_files/figure-html/visual_comparison-1.png" width="672" /></p>
<p>From this we can see that in terms of predictive power both models behave very similarly, and in practical terms both are going to give directionally similar answers. We can then use this result to estimate the impact of each one of these variables for the model.</p>
<p>Finally, a word of caution about the algorithm “Port”. Port comes from a library using nl2sol solution. In layman terms, tries to find lowest residual possible using something called nonlinear least-squares algorithm. As many other non-linear algorithms, the starting conditions could impact the results of the convergence. I personally recommend 3 approaches to lower the chances to encounter local minima.</p>
<ul>
<li><p>Test different innitial conditions. By randomly picking different starting conditions we can evaluate if the results converge to similar coefficients.</p></li>
<li><p>Test cross validation. To evaluate the variance of the estimated coefficients.</p></li>
<li><p>Use alternative methods, such as Bayes and Lasso to see if all models reach the same conclusions.</p></li>
</ul>
</div>

</div>


  <footer class="container">
  <div class="container navigation no-print">
  <h2>Navigation</h2>
  
  


</div>

  

</footer>

</article>
      <footer id="main-footer" class="container main_footer">
  

  <div class="container nav foot no-print">
  

  <a class="toplink" href="#">back to top</a>

</div>

  <div class="container credits">
  
<div class="container footline">
  
  It&rsquo;s hard but you gotta keep trying


</div>


  
<div class="container copyright">
  
  &copy; 2019 Alvaro Aguado


</div>


</div>

</footer>

    </main>
    


<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



    
  </body>
</html>

